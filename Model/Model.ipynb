{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1dd712f-f736-4f5a-9084-093ab7ea6970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json, expr, col, when, udf, unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a587266d-450c-4ab8-ba19-0f34d27a4872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/22 11:41:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/22 11:41:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/22 11:41:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "##Configuration\n",
    "\n",
    "#config the connector jar file\n",
    "spark = (SparkSession.builder.appName(\"SimpleSparkJob\").master(\"spark://34.142.194.212:7077\")\n",
    "         .config(\"spark.jars\", \"/opt/spark/jars/gcs-connector-latest-hadoop2.jar\")\n",
    "         .config(\"spark.executor.memory\", \"2G\")  #excutor excute only 2G\n",
    "        .config(\"spark.driver.memory\",\"4G\") \n",
    "        .config(\"spark.executor.cores\",\"1\") #Cluster use only 3 cores to excute as it has 3 server\n",
    "        .config(\"spark.python.worker.memory\",\"1G\") # each worker use 1G to excute\n",
    "        .config(\"spark.driver.maxResultSize\",\"3G\") #Maximum size of result is 3G\n",
    "        .config(\"spark.kryoserializer.buffer.max\",\"1024M\")\n",
    "         .getOrCreate())\n",
    "#config the credential to identify the google cloud hadoop file \n",
    "spark.conf.set(\"google.cloud.auth.service.account.json.keyfile\",\"/opt/bucket_connector/lucky-wall-393304-3fbad5f3943c.json\")\n",
    "spark._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.gs.auth.service.account.enable', 'true')\n",
    "\n",
    "## Connect to the file in Google Bucket with Spark\n",
    "\n",
    "\n",
    "# spark.show()\n",
    "\n",
    "# spark.stop # Ending spark job\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97499962-da7b-4689-9c41-00f106260faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path=f\"gs://it4043e-it5384/it4043e/it4043e_group7_problem1/final_final_preprocess\"\n",
    "tweet_final = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"wholeFile\", \"true\") \\\n",
    ".load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc5ecc3-daef-46ec-865e-615d8c3628e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['username',\n",
       " 'tweet_text',\n",
       " 'datetime_timestamp',\n",
       " 'tweet_id',\n",
       " 'discussion_link',\n",
       " 'images',\n",
       " 'video_preview',\n",
       " 'user_location',\n",
       " 'user_description',\n",
       " 'date',\n",
       " 'time',\n",
       " 'language_tweet',\n",
       " 'language_description']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eef8d19f-e0c4-46ee-8a90-296abfd1ac30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_description_fill_value = \"\"\n",
    "\n",
    "# Specify the fill value for time\n",
    "time_fill_value = \"00:00:00\"\n",
    "\n",
    "# Fill null values in user_description and time columns\n",
    "tweet_final = tweet_final.na.fill({\"user_description\": user_description_fill_value, \n",
    "                                   \"time\": time_fill_value, \n",
    "                                   \"language_tweet\": user_description_fill_value, \n",
    "                                   \"language_description\": user_description_fill_value,\n",
    "                                  \"user_location\": user_description_fill_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1e0f0f-fcd8-445e-aea6-c0292713413d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming `tweet_final` is your DataFrame\n",
    "tweet_final = tweet_final.filter(tweet_final['username'].isNotNull())\n",
    "tweet_final = tweet_final.filter(tweet_final['tweet_text'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913cc3b4-c03c-444d-aa8a-d7808ac4634c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------+--------+---------------+------+-------------+-------------+----------------+----+----+--------------+--------------------+\n",
      "|username|tweet_text|datetime_timestamp|tweet_id|discussion_link|images|video_preview|user_location|user_description|date|time|language_tweet|language_description|\n",
      "+--------+----------+------------------+--------+---------------+------+-------------+-------------+----------------+----+----+--------------+--------------------+\n",
      "|       0|         0|                 0|       0|              0|     0|            0|            0|               0|   0|   0|             0|                   0|\n",
      "+--------+----------+------------------+--------+---------------+------+-------------+-------------+----------------+----+----+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "tweet_final.select([count(when(col(c).isNull(), c)).alias(c) for c in tweet_final.columns]\n",
    "   ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d948002-5dcc-4b8a-9f80-ae179ba86aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql import functions as F\n",
    "grouped_df = tweet_final.groupBy(\"username\", \"user_location\").agg(\n",
    "    F.expr(\"percentile_approx(time, 0.5)\").alias(\"median_time\"),\n",
    "    F.expr(\"first(language_tweet, true)\").alias(\"mode_tweet_language\"),\n",
    "    F.expr(\"first(language_description, true)\").alias(\"mode_description_language\"),\n",
    "    F.concat_ws(\" \", F.collect_list(\"user_description\")).alias(\"concatenated_user_description\"),\n",
    "    F.concat_ws(\" \", F.collect_list(\"tweet_text\")).alias(\"concatenated_tweet_text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e317b6c-3d7e-42ad-b787-6b4d78e6d73c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+-------------------+-------------------------+-----------------------------+-----------------------+\n",
      "|            username|       user_location|median_time|mode_tweet_language|mode_description_language|concatenated_user_description|concatenated_tweet_text|\n",
      "+--------------------+--------------------+-----------+-------------------+-------------------------+-----------------------------+-----------------------+\n",
      "|                   0| Santa Fe, Argentina|    56120.0|            English|                         |                             |   #BTC fixes this h...|\n",
      "|                   0|                 USA|    57119.0|            English|                  English|         #BTC $ETH \\n$QNT ...|   Digital real esta...|\n",
      "|            00100100|                    |    56568.0|            English|                  English|         i like to talk ab...|   Eventually, you w...|\n",
      "|                0021|                 USA|    62805.0|            English|                  English|         Keeping an eye on...|   I spy the Bitcoin...|\n",
      "|             0060sol|            pfp: mcd|    32840.0|            English|                  English|         Cryptocurrency In...|   Already priced in...|\n",
      "|      00michaeltalty|            new york|    47298.0|            English|                         |                          ...|   @SDPInstitute St ...|\n",
      "|             00press|                  PA|    65719.0|            English|                   French|         Double Aught pres...|   Can you imagine t...|\n",
      "|01000001011100100...|                    |    57470.0|            English|                  English|         #BTC fan! So by e...|   @DocumentingBTC T...|\n",
      "|01001010011011110...|        AquÃ­ y ahora|    78604.0|            English|                   French|          Dialectical, spe...|   JJPL: BTC\\n\\nAppa...|\n",
      "|         01010010101|                    |    28032.0|            English|                         |                             |   $btc $cate #btc #...|\n",
      "|01029387382747338...|                    |    32062.0|            English|                  English|                       crypto|   #Bitcoin Remember...|\n",
      "|             010Coin|Rotterdam, Nederland|    39007.0|            English|                  English|         Shaping my future...|   I made this #Bitc...|\n",
      "|  01105bnbmeta475eth|         Philippines|    59197.0|            English|                  English|         A Filipino findin...|   This crypto crash...|\n",
      "|   01123581321345589|29.9792Â° N, 31.13...|    62962.0|            English|                  English|         Timechain partial...|   @biancoresearch S...|\n",
      "|     012letsatsantsa|                    |      560.0|            English|                  English|         Bodhisattva shoul...|   #APE Coin To $30!...|\n",
      "|         01818210973|                    |    44791.0|            English|                  English|         There is no sad e...|   @airdropinspect W...|\n",
      "|             0194eth|Training in Whis'...|    54341.0|             French|                  English|         #Bitcoin #Ethereu...|   Can #btc get to $...|\n",
      "|           019950ETH|           Stormwind|    56292.0|            English|                  English|         Trying to build t...|   @CryptoInsider23 ...|\n",
      "|           01CAPITAL|                    |     8331.0|            English|                  English|         Professional asse...|   #Bitcoin doesn't ...|\n",
      "|            02072021|       On Blockchain|    44723.0|            English|                         |         gm gm gm gm C8H10...|   @MatiGreenspan On...|\n",
      "+--------------------+--------------------+-----------+-------------------+-------------------------+-----------------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce023ac3-bb1b-4d77-97d4-12e2396e2e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- user_location: string (nullable = false)\n",
      " |-- median_time: double (nullable = true)\n",
      " |-- mode_tweet_language: string (nullable = true)\n",
      " |-- mode_description_language: string (nullable = true)\n",
      " |-- concatenated_user_description: string (nullable = false)\n",
      " |-- concatenated_tweet_text: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e9ed75f-908b-4775-8347-be7e52f28dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Assuming 'tweet_final' is your DataFrame\n",
    "\n",
    "# Tokenize text columns\n",
    "tokenizer = Tokenizer(inputCol='username', outputCol='username_tokens')\n",
    "grouped_df = tokenizer.transform(grouped_df)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='concatenated_tweet_text', outputCol='tweet_tokens')\n",
    "grouped_df = tokenizer.transform(grouped_df)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='concatenated_user_description', outputCol='description_tokens')\n",
    "grouped_df = tokenizer.transform(grouped_df)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='mode_tweet_language', outputCol='lang_tweet_tokens')\n",
    "grouped_df = tokenizer.transform(grouped_df)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='mode_description_language', outputCol='lang_description_tokens')\n",
    "grouped_df = tokenizer.transform(grouped_df)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='user_location', outputCol='user_location_tokens')\n",
    "grouped_df = tokenizer.transform(grouped_df)\n",
    "\n",
    "#tokenizer = Tokenizer(inputCol='time_interval', outputCol='time_interval_tokens')\n",
    "#tweet_final = tokenizer.transform(tweet_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb6b04b1-99ff-4c81-9256-399ff9ba7a55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"hashingTF = HashingTF(inputCol='time_interval_tokens', outputCol='time_interval_tf')\\ntweet_final = hashingTF.transform(tweet_final)\\nidf = IDF(inputCol='time_interval_tf', outputCol='time_interval_tfidf')\\ntweet_final = idf.fit(tweet_final).transform(tweet_final)\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply TF-IDF to the tokenized text columns\n",
    "hashingTF = HashingTF(inputCol='username_tokens', outputCol='username_tf')\n",
    "grouped_df = hashingTF.transform(grouped_df)\n",
    "idf = IDF(inputCol='username_tf', outputCol='username_tfidf')\n",
    "grouped_df = idf.fit(grouped_df).transform(grouped_df)\n",
    "\n",
    "hashingTF = HashingTF(inputCol='tweet_tokens', outputCol='tweet_tf')\n",
    "grouped_df = hashingTF.transform(grouped_df)\n",
    "idf = IDF(inputCol='tweet_tf', outputCol='tweet_tfidf')\n",
    "grouped_df = idf.fit(grouped_df).transform(grouped_df)\n",
    "\n",
    "hashingTF = HashingTF(inputCol='description_tokens', outputCol='description_tf')\n",
    "grouped_df = hashingTF.transform(grouped_df)\n",
    "idf = IDF(inputCol='description_tf', outputCol='description_tfidf')\n",
    "grouped_df = idf.fit(grouped_df).transform(grouped_df)\n",
    "\n",
    "hashingTF = HashingTF(inputCol='lang_tweet_tokens', outputCol='tweet_lang_tf')\n",
    "grouped_df = hashingTF.transform(grouped_df)\n",
    "idf = IDF(inputCol='tweet_lang_tf', outputCol='tweet_lang_tfidf')\n",
    "grouped_df = idf.fit(grouped_df).transform(grouped_df)\n",
    "\n",
    "hashingTF = HashingTF(inputCol='lang_description_tokens', outputCol='description_lang_tf')\n",
    "grouped_df = hashingTF.transform(grouped_df)\n",
    "idf = IDF(inputCol='description_lang_tf', outputCol='description_lang_tfidf')\n",
    "grouped_df = idf.fit(grouped_df).transform(grouped_df)\n",
    "\n",
    "hashingTF = HashingTF(inputCol='user_location_tokens', outputCol='user_location_tf')\n",
    "grouped_df = hashingTF.transform(grouped_df)\n",
    "idf = IDF(inputCol='user_location_tf', outputCol='user_location_tfidf')\n",
    "grouped_df = idf.fit(grouped_df).transform(grouped_df)\n",
    "\n",
    "'''hashingTF = HashingTF(inputCol='time_interval_tokens', outputCol='time_interval_tf')\n",
    "tweet_final = hashingTF.transform(tweet_final)\n",
    "idf = IDF(inputCol='time_interval_tf', outputCol='time_interval_tfidf')\n",
    "tweet_final = idf.fit(tweet_final).transform(tweet_final)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f11664d4-e712-4ade-a44c-1c6c310a0804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert 'date' and 'time' to Unix timestamps\n",
    "#tweet_final = tweet_final.withColumn('date', unix_timestamp('date', 'yy-MM-dd').cast('double'))\n",
    "tweet_final = tweet_final.withColumn('time', unix_timestamp('time', 'HH:mm:ss').cast('double'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b51fc72-ad9a-4ae2-82d4-f3e70e3f6456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "min_time = tweet_final.agg({\"time\": \"min\"}).collect()[0][\"min(time)\"]\n",
    "max_time = tweet_final.agg({\"time\": \"max\"}).collect()[0][\"max(time)\"]\n",
    "\n",
    "normalized_time_expr = (col(\"time\") - min_time) / (max_time - min_time)\n",
    "tweet_final = tweet_final.withColumn(\"normalized_time\", normalized_time_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f203984a-6a44-42f8-b588-afdc2011c7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|    normalized_time|   time|\n",
      "+-------------------+-------+\n",
      "| 0.6641975022859061|57386.0|\n",
      "| 0.7181911827683191|62051.0|\n",
      "|0.32502691003368095|28082.0|\n",
      "|  0.506857718260628|43792.0|\n",
      "|0.30815171471892033|26624.0|\n",
      "| 0.9257977522887997|79988.0|\n",
      "| 0.5400872695285825|46663.0|\n",
      "| 0.5516614775633977|47663.0|\n",
      "|  0.194863366474149|16836.0|\n",
      "| 0.4181414136737694|36127.0|\n",
      "| 0.1156842093079781| 9995.0|\n",
      "| 0.2145626685494045|18538.0|\n",
      "| 0.6755518003680598|58367.0|\n",
      "|0.39498142339610415|34126.0|\n",
      "|0.15673792520746768|13542.0|\n",
      "| 0.8597784696582136|74284.0|\n",
      "| 0.9130198266183637|78884.0|\n",
      "| 0.3583722033819836|30963.0|\n",
      "| 0.6633641593073993|57314.0|\n",
      "| 0.5343464623433142|46167.0|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweet_final.select('normalized_time', 'time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdd1a02c-41e6-48e5-b608-599f4e22ff39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assemble features into a single vector column\n",
    "\n",
    "feature_cols = ['username_tfidf',\n",
    "                'tweet_tfidf',\n",
    "                'description_tfidf',\n",
    "                'tweet_lang_tfidf',\n",
    "                'description_lang_tfidf',\n",
    "                'median_time', \n",
    "                'user_location_tfidf']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid='skip')\n",
    "data = assembler.transform(grouped_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "367141f0-0855-4dac-b0bd-8bc29531c240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- user_location: string (nullable = false)\n",
      " |-- median_time: double (nullable = true)\n",
      " |-- mode_tweet_language: string (nullable = true)\n",
      " |-- mode_description_language: string (nullable = true)\n",
      " |-- concatenated_user_description: string (nullable = false)\n",
      " |-- concatenated_tweet_text: string (nullable = false)\n",
      " |-- username_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tweet_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- description_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lang_tweet_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lang_description_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_location_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- username_tf: vector (nullable = true)\n",
      " |-- username_tfidf: vector (nullable = true)\n",
      " |-- tweet_tf: vector (nullable = true)\n",
      " |-- tweet_tfidf: vector (nullable = true)\n",
      " |-- description_tf: vector (nullable = true)\n",
      " |-- description_tfidf: vector (nullable = true)\n",
      " |-- tweet_lang_tf: vector (nullable = true)\n",
      " |-- tweet_lang_tfidf: vector (nullable = true)\n",
      " |-- description_lang_tf: vector (nullable = true)\n",
      " |-- description_lang_tfidf: vector (nullable = true)\n",
      " |-- user_location_tf: vector (nullable = true)\n",
      " |-- user_location_tfidf: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76314199-6220-4390-81c4-c493d38a108a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/22 12:34:08 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:36:37 WARN DAGScheduler: Broadcasting large task binary with size 124.7 MiB\n",
      "23/12/22 12:37:00 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:39:30 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:39:48 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:40:18 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:40:38 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:41:09 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:41:31 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:41:48 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/12/22 12:42:09 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:44:34 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:44:57 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:45:19 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:45:42 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:46:07 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:46:30 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:46:53 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:47:16 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:47:38 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:48:01 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:48:27 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:48:57 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:49:30 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:50:03 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:50:26 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:50:50 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:51:13 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:51:36 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:51:59 WARN DAGScheduler: Broadcasting large task binary with size 124.8 MiB\n",
      "23/12/22 12:52:44 WARN DAGScheduler: Broadcasting large task binary with size 184.8 MiB\n",
      "23/12/22 12:55:37 WARN DAGScheduler: Broadcasting large task binary with size 184.7 MiB\n",
      "23/12/22 12:56:04 WARN DAGScheduler: Broadcasting large task binary with size 184.8 MiB\n",
      "23/12/22 12:58:48 WARN DAGScheduler: Broadcasting large task binary with size 84.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.6877878692101088\n",
      "Cluster Centers: \n",
      "[0.        0.        0.0002126 ... 0.        0.        0.       ]\n",
      "[0.00012835 0.         0.00023389 ... 0.         0.         0.        ]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.         0.         0.00014424 ... 0.         0.         0.        ]\n",
      "[0.00000000e+00 1.00238424e-04 9.13284930e-05 ... 1.00238424e-04\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Set the number of clusters\n",
    "kmeans = KMeans().setK(5).setSeed(1)\n",
    "\n",
    "# Fit the model\n",
    "model = kmeans.fit(data)\n",
    "\n",
    "# Predict clusters\n",
    "predictions = model.transform(data)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette_score = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette score: {silhouette_score}\")\n",
    "\n",
    "# Shows the result\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b59214fb-3fab-4ffb-aafd-e6266076f014",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/21 12:14:58 WARN DAGScheduler: Broadcasting large task binary with size 163.4 MiB\n",
      "23/12/21 12:15:48 WARN DAGScheduler: Broadcasting large task binary with size 80.1 MiB\n",
      "[Stage 660:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davies-Bouldin Index: -0.24849720185441587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator1 = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"features\", metricName=\"davies_bouldin\")\n",
    "db_index = evaluator.evaluate(predictions)\n",
    "print(f\"Davies-Bouldin Index: {db_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9880cc9f-3330-44fb-9a80-56b0a11c5feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/22 13:02:44 WARN DAGScheduler: Broadcasting large task binary with size 84.2 MiB\n",
      "[Stage 136:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+----------+\n",
      "|            username|median_time|       user_location|prediction|\n",
      "+--------------------+-----------+--------------------+----------+\n",
      "|                   0|    56120.0| Santa Fe, Argentina|         4|\n",
      "|                   0|    57119.0|                 USA|         4|\n",
      "|            00100100|    56568.0|                    |         4|\n",
      "|                0021|    62805.0|                 USA|         4|\n",
      "|             0060sol|    32840.0|            pfp: mcd|         3|\n",
      "|      00michaeltalty|    47298.0|            new york|         2|\n",
      "|             00press|    65719.0|                  PA|         4|\n",
      "|01000001011100100...|    57470.0|                    |         4|\n",
      "|01001010011011110...|    78604.0|        AquÃ­ y ahora|         1|\n",
      "|         01010010101|    28032.0|                    |         3|\n",
      "|01029387382747338...|    32062.0|                    |         3|\n",
      "|             010Coin|    39007.0|Rotterdam, Nederland|         2|\n",
      "|  01105bnbmeta475eth|    59197.0|         Philippines|         4|\n",
      "|   01123581321345589|    62962.0|29.9792Â° N, 31.13...|         4|\n",
      "|     012letsatsantsa|      560.0|                    |         0|\n",
      "|         01818210973|    44791.0|                    |         2|\n",
      "|             0194eth|    54341.0|Training in Whis'...|         2|\n",
      "|           019950ETH|    56292.0|           Stormwind|         4|\n",
      "|           01CAPITAL|     8331.0|                    |         0|\n",
      "|            02072021|    44723.0|       On Blockchain|         2|\n",
      "|          04_danilka|    61119.0|             Ð£ÐºÑ€Ð°Ð¸Ð½Ð°|         4|\n",
      "|        0711_Bitcoin|    39557.0|                    |         2|\n",
      "|         0711_itcoin|    42019.0|                    |         2|\n",
      "|             0870eth|    65154.0|       SandVault.eth|         4|\n",
      "|       0Fundamentals|    35784.0|           Stockholm|         3|\n",
      "|               0Mike|    66358.0|                    |         4|\n",
      "|                0NFT|    74503.0|                    |         1|\n",
      "|           0PTIMUSG0|    28629.0|           Cybertron|         3|\n",
      "|            0Percent|    50560.0|      Miami, Florida|         2|\n",
      "|              0Xhope|    39547.0|         Every where|         2|\n",
      "|                 0db|    11521.0|                    |         0|\n",
      "|                0hmm|    82516.0|                    |         1|\n",
      "|             0liNaea|    42734.0|                    |         2|\n",
      "|0rang3destroyerof...|    34853.0|            mempool |         3|\n",
      "|0rang3destroyerof...|    49171.0|            mempool |         2|\n",
      "|        0rangepilled|    28275.0|           Australia|         3|\n",
      "|               0sats|    58749.0|      in your wallet|         4|\n",
      "|                  0x|    59889.0|                    |         4|\n",
      "|                  0x|    52972.0|         Blockchain |         2|\n",
      "|                  0x|    70873.0|       The Metaverse|         1|\n",
      "|                0x01|    55650.0|             Toronto|         4|\n",
      "|              0x01bD|    59023.0|                    |         4|\n",
      "|          0x0smaneth|    86235.0|                    |         1|\n",
      "|            0x1337ff|    58055.0|              France|         4|\n",
      "|            0x146eth|    34307.0|Scotland, United ...|         3|\n",
      "|            0x69Sinu|    85226.0|                    |         1|\n",
      "|                 0x8|    38761.0|           ToTheMoon|         3|\n",
      "|           0xAI82517|    51653.0|            Maldives|         2|\n",
      "|          0xAlineeth|      505.0|                    |         0|\n",
      "|          0xAllin168|    59195.0|                    |         4|\n",
      "+--------------------+-----------+--------------------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select('username', 'median_time', 'user_location', 'prediction').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7eafc48d-60b0-40ef-83b0-ef87aed45e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/21 11:34:27 WARN DAGScheduler: Broadcasting large task binary with size 80.1 MiB\n",
      "[Stage 515:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------------------+--------------+----------+\n",
      "|      username|    time|language_description|language_tweet|prediction|\n",
      "+--------------+--------+--------------------+--------------+----------+\n",
      "|0xSuperTrooper|23:32:00|             English|       English|         0|\n",
      "+--------------+--------+--------------------+--------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions['username'] == '0xSuperTrooper').select('username', 'time', 'language_description','language_tweet', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7312a6b0-1a95-437c-9c24-5f7b778bfd66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/22 13:05:32 WARN DAGScheduler: Broadcasting large task binary with size 184.8 MiB\n",
      "23/12/22 13:07:55 WARN DAGScheduler: Broadcasting large task binary with size 184.7 MiB\n",
      "[Stage 142:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|         1| 95866|\n",
      "|         3| 78319|\n",
      "|         4|123011|\n",
      "|         2|116047|\n",
      "|         0| 53123|\n",
      "+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.groupBy(predictions.prediction).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68ccfad1-b863-4b59-9c00-7a3f3d3d985f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/22 13:13:45 WARN DAGScheduler: Broadcasting large task binary with size 84.2 MiB\n",
      "[Stage 151:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|            username|median_time|       user_location|\n",
      "+--------------------+-----------+--------------------+\n",
      "|01001010011011110...|    78604.0|        AquÃ­ y ahora|\n",
      "|                0NFT|    74503.0|                    |\n",
      "|                0hmm|    82516.0|                    |\n",
      "|                  0x|    70873.0|       The Metaverse|\n",
      "|          0x0smaneth|    86235.0|                    |\n",
      "|            0x69Sinu|    85226.0|                    |\n",
      "|         0xComÎžtaeth|    79861.0|                    |\n",
      "|        0xCryptology|    78426.0|  EskiÅŸehir, TÃ¼rkiye|\n",
      "|        0xDigitalOil|    72980.0|                    |\n",
      "|           0xDoodles|    71776.0| Ethereum Blockchain|\n",
      "|           0xEvoleth|    77331.0|            Da Swamp|\n",
      "|               0xHex|    72548.0|                    |\n",
      "|   0xLamborghiniLady|    71059.0|                    |\n",
      "|         0xLeMistral|    74174.0|   Le Mistral Palace|\n",
      "|             0xLongs|    81405.0|                    |\n",
      "|       0xLugrinhot3v|    71567.0|                    |\n",
      "|         0xLungineth|    74735.0|     Los Angeles, CA|\n",
      "|         0xMbvissers|    75848.0|                    |\n",
      "|             0xMitch|    75671.0|                    |\n",
      "|              0xMonk|    81331.0|        Planet Earth|\n",
      "|             0xQuant|    74223.0|               Paris|\n",
      "|                0xRB|    79444.0| 0101010101101010101|\n",
      "|          0xShulleth|    80731.0|           Hong Kong|\n",
      "|            0xSil3nt|    74124.0|        Konohagakure|\n",
      "|            0xStormz|    77370.0|           Miami, FL|\n",
      "|              0xToit|    73017.0|                    |\n",
      "|          0xalieneth|    76871.0|              WAGMI |\n",
      "|             0xanima|    83625.0|                    |\n",
      "|       0xcristianeth|    72954.0|          California|\n",
      "|0xduzaFollowforda...|    81759.0|                    |\n",
      "|      0xfordcommaeth|    79953.0|              Denver|\n",
      "|          0xfudysnom|    69586.0|                    |\n",
      "|      0xmothpainpain|    69698.0|                    |\n",
      "|        0xpapijhaytu|    74559.0|                    |\n",
      "|               1000X|    77330.0|                    |\n",
      "|        1000XPRESALE|    74410.0|                    |\n",
      "|               1000x|    72804.0|     ASTRO Galazy ðŸŒŒ|\n",
      "|           100BFmcom|    71828.0| BINANCE SMART CHAIN|\n",
      "|100TÏÎ¹ÏƒÎµÎºÎ±Ï„Î¿Î¼Î¼Ï…ÏÎ¯...|    70296.0|                    |\n",
      "|          100XCRYPTO|    69699.0|                    |\n",
      "|100gemsGRVVGXMATI...|    69994.0|                    |\n",
      "|            100state|    72102.0|             Madison|\n",
      "|        100xGEMWhale|    82169.0|       Paris, France|\n",
      "|          100xcrypto|    71400.0|                    |\n",
      "|      1075investment|    82217.0|   Ä°stanbul, TÃ¼rkiye|\n",
      "|  10kCryptoChallenge|    78006.0|                    |\n",
      "|           10nixenK0|    79884.0|               Spain|\n",
      "|          11DAYBREAK|    73084.0|           Worldwide|\n",
      "|                 122|    76800.0|Walnut Hills, Cin...|\n",
      "|            123kings|    69964.0|Central Region, S...|\n",
      "+--------------------+-----------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.filter((predictions.prediction == 1) & (predictions.user_location.isNotNull())).select('username', 'median_time', 'user_location').show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7b93f61-f30c-4279-8416-deb360df01b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/22 13:25:13 WARN DAGScheduler: Broadcasting large task binary with size 184.8 MiB\n",
      "23/12/22 13:28:08 WARN DAGScheduler: Broadcasting large task binary with size 184.7 MiB\n",
      "[Stage 175:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|  user_location|count|\n",
      "+---------------+-----+\n",
      "|               |44856|\n",
      "|  United States| 1287|\n",
      "|      Metaverse|  652|\n",
      "|Los Angeles, CA|  568|\n",
      "|London, England|  540|\n",
      "|            USA|  491|\n",
      "|         Canada|  448|\n",
      "| United Kingdom|  429|\n",
      "|  New York, USA|  420|\n",
      "|          Earth|  407|\n",
      "|California, USA|  406|\n",
      "|        Nigeria|  336|\n",
      "|      Australia|  280|\n",
      "| Lagos, Nigeria|  256|\n",
      "|      Miami, FL|  256|\n",
      "|   New York, NY|  251|\n",
      "|           Moon|  249|\n",
      "|   Florida, USA|  236|\n",
      "|         London|  233|\n",
      "|          India|  229|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "predictions.filter(predictions.prediction == 1).groupBy(\"user_location\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b00050a2-c880-4ec6-9eaf-92768cc048f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/22 13:32:36 WARN DAGScheduler: Broadcasting large task binary with size 24.1 MiB\n",
      "23/12/22 13:33:36 WARN DAGScheduler: Broadcasting large task binary with size 24.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|mode_tweet_language|\n",
      "+-------------------+\n",
      "|            Turkish|\n",
      "|            English|\n",
      "|            Spanish|\n",
      "|           Kurmanci|\n",
      "|             French|\n",
      "|                   |\n",
      "|             German|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select('mode_tweet_language').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2290e0c-291d-4065-81eb-359ab587d4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- user_location: string (nullable = false)\n",
      " |-- median_time: double (nullable = true)\n",
      " |-- mode_tweet_language: string (nullable = true)\n",
      " |-- mode_description_language: string (nullable = true)\n",
      " |-- concatenated_user_description: string (nullable = false)\n",
      " |-- concatenated_tweet_text: string (nullable = false)\n",
      " |-- username_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tweet_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- description_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lang_tweet_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lang_description_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_location_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- username_tf: vector (nullable = true)\n",
      " |-- username_tfidf: vector (nullable = true)\n",
      " |-- tweet_tf: vector (nullable = true)\n",
      " |-- tweet_tfidf: vector (nullable = true)\n",
      " |-- description_tf: vector (nullable = true)\n",
      " |-- description_tfidf: vector (nullable = true)\n",
      " |-- tweet_lang_tf: vector (nullable = true)\n",
      " |-- tweet_lang_tfidf: vector (nullable = true)\n",
      " |-- description_lang_tf: vector (nullable = true)\n",
      " |-- description_lang_tfidf: vector (nullable = true)\n",
      " |-- user_location_tf: vector (nullable = true)\n",
      " |-- user_location_tfidf: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/22 13:40:31 WARN DAGScheduler: Broadcasting large task binary with size 258.1 MiB\n",
      "23/12/22 13:41:03 WARN TaskSetManager: Lost task 0.0 in stage 184.2 (TID 618) (10.148.0.23 executor 4): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:86)\n",
      "\tat java.base/java.lang.StringBuilder.<init>(StringBuilder.java:116)\n",
      "\tat com.fasterxml.jackson.core.util.TextBuffer.contentsAsString(TextBuffer.java:487)\n",
      "\tat com.fasterxml.jackson.core.io.SegmentedStringWriter.getAndClear(SegmentedStringWriter.java:99)\n",
      "\tat com.fasterxml.jackson.databind.ObjectWriter.writeValueAsString(ObjectWriter.java:1141)\n",
      "\tat org.json4s.jackson.JsonMethods.pretty(JsonMethods.scala:60)\n",
      "\tat org.json4s.jackson.JsonMethods.pretty$(JsonMethods.scala:58)\n",
      "\tat org.json4s.jackson.JsonMethods$.pretty(JsonMethods.scala:71)\n",
      "\tat org.apache.spark.sql.types.DataType.prettyJson(DataType.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$init$3(ParquetWriteSupport.scala:137)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1513/0x0000000840ca5040.apply(Unknown Source)\n",
      "\tat org.apache.spark.internal.Logging.logInfo(Logging.scala:60)\n",
      "\tat org.apache.spark.internal.Logging.logInfo$(Logging.scala:59)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.logInfo(ParquetWriteSupport.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.init(ParquetWriteSupport.scala:140)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:478)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec$$Lambda$743/0x0000000840722040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$744/0x0000000840722840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\n",
      "23/12/22 13:41:09 ERROR TaskSchedulerImpl: Lost executor 4 on 10.148.0.23: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/12/22 13:41:09 WARN TaskSetManager: Lost task 2.0 in stage 184.2 (TID 620) (10.148.0.23 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/12/22 13:41:11 ERROR TaskSchedulerImpl: Lost executor 5 on 10.148.0.23: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/12/22 13:41:11 WARN TaskSetManager: Lost task 0.1 in stage 184.2 (TID 621) (10.148.0.23 executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/12/22 13:41:21 WARN TaskSetManager: Lost task 0.2 in stage 184.2 (TID 622) (10.148.0.23 executor 6): FetchFailed(null, shuffleId=57, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 57 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1739)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1686)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1685)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1685)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1327)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/12/22 13:41:23 WARN TaskSetManager: Lost task 2.1 in stage 184.2 (TID 623) (10.148.0.23 executor 7): FetchFailed(null, shuffleId=57, mapIndex=-1, mapId=-1, reduceId=63, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 57 partition 63\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1739)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1686)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1685)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1685)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1327)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/12/22 13:41:26 WARN TaskSetManager: Lost task 1.1 in stage 184.2 (TID 624) (10.148.0.23 executor 6): FetchFailed(null, shuffleId=57, mapIndex=-1, mapId=-1, reduceId=31, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 57 partition 31\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1739)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1686)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1685)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1685)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1327)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/12/22 13:42:02 WARN DAGScheduler: Broadcasting large task binary with size 258.1 MiB\n",
      "23/12/22 13:42:33 WARN TaskSetManager: Lost task 0.0 in stage 184.3 (TID 630) (10.148.0.23 executor 6): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:86)\n",
      "\tat java.base/java.lang.StringBuilder.<init>(StringBuilder.java:116)\n",
      "\tat com.fasterxml.jackson.core.util.TextBuffer.contentsAsString(TextBuffer.java:487)\n",
      "\tat com.fasterxml.jackson.core.io.SegmentedStringWriter.getAndClear(SegmentedStringWriter.java:99)\n",
      "\tat com.fasterxml.jackson.databind.ObjectWriter.writeValueAsString(ObjectWriter.java:1141)\n",
      "\tat org.json4s.jackson.JsonMethods.pretty(JsonMethods.scala:60)\n",
      "\tat org.json4s.jackson.JsonMethods.pretty$(JsonMethods.scala:58)\n",
      "\tat org.json4s.jackson.JsonMethods$.pretty(JsonMethods.scala:71)\n",
      "\tat org.apache.spark.sql.types.DataType.prettyJson(DataType.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$init$3(ParquetWriteSupport.scala:137)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$1514/0x0000000840ca5040.apply(Unknown Source)\n",
      "\tat org.apache.spark.internal.Logging.logInfo(Logging.scala:60)\n",
      "\tat org.apache.spark.internal.Logging.logInfo$(Logging.scala:59)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.logInfo(ParquetWriteSupport.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.init(ParquetWriteSupport.scala:140)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:478)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec$$Lambda$743/0x0000000840722040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$744/0x0000000840722840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\n",
      "23/12/22 13:42:38 ERROR TaskSchedulerImpl: Lost executor 6 on 10.148.0.23: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/12/22 13:42:38 WARN TaskSetManager: Lost task 2.0 in stage 184.3 (TID 632) (10.148.0.23 executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/12/22 13:42:45 ERROR TaskSchedulerImpl: Lost executor 7 on 10.148.0.23: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/12/22 13:42:45 WARN TaskSetManager: Lost task 2.1 in stage 184.3 (TID 633) (10.148.0.23 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/12/22 13:42:54 WARN TaskSetManager: Lost task 2.2 in stage 184.3 (TID 635) (10.148.0.23 executor 9): FetchFailed(null, shuffleId=57, mapIndex=-1, mapId=-1, reduceId=63, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 57 partition 63\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1739)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1686)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1685)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1685)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1327)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/12/22 13:42:54 ERROR FileFormatWriter: Aborting job 79dca8b2-14a4-4030-adcc-8234ee070fc1.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 184 (parquet at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason:\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 57 partition 63\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1739)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1686)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1685)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1685)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1327)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2019)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3042)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/12/22 13:42:58 WARN TaskSetManager: Lost task 0.1 in stage 184.3 (TID 636) (10.148.0.23 executor 9): FetchFailed(null, shuffleId=57, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 57 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1739)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1686)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1685)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1685)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1327)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/12/22 13:42:58 WARN TaskSetManager: Lost task 1.1 in stage 184.3 (TID 634) (10.148.0.23 executor 8): FetchFailed(null, shuffleId=57, mapIndex=-1, mapId=-1, reduceId=31, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 57 partition 31\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1739)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1686)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1685)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1685)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1327)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac7a9c5d-c861-4504-8697-f4fe8aaa5532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/22 13:44:55 WARN DAGScheduler: Broadcasting large task binary with size 84.4 MiB\n",
      "23/12/22 14:49:42 ERROR TaskSchedulerImpl: Lost executor 8 on 10.148.0.23: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/12/22 14:49:43 ERROR TaskSchedulerImpl: Lost executor 9 on 10.148.0.23: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/12/22 15:43:33 WARN TransportChannelHandler: Exception in connection from /34.142.194.212:7077\n",
      "java.io.IOException: Connection reset by peer\n",
      "\tat java.base/sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:233)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:223)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:356)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/12/22 15:43:33 WARN StandaloneAppClient$ClientEndpoint: Could not connect to 34.142.194.212:7077: java.io.IOException: Connection reset by peer\n",
      "23/12/22 15:43:33 WARN StandaloneAppClient$ClientEndpoint: Connection to 10.148.0.21:7077 failed; waiting for master to reconnect...\n",
      "23/12/22 15:43:33 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n"
     ]
    }
   ],
   "source": [
    "predictions.select('username', 'user_location', 'median_time', 'mode_tweet_language','mode_description_language', 'concatenated_user_description', 'concatenated_tweet_text', 'prediction').write.parquet(\"gs://it4043e-it5384/it4043e/it4043e_group7_problem1/prediction\",mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784e3fe8-6efc-4c52-a044-4e703ceff8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2af0158-4790-41e0-8fab-2a40ed7486f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group07",
   "language": "python",
   "name": "group07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
